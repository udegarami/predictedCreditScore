{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36830dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load applications data\n",
    "path = 'dataset/'\n",
    "train = pd.read_csv(path + 'application_train.csv')\n",
    "test = pd.read_csv(path + 'application_test.csv')\n",
    "\n",
    "train.sample(frac=0.1, replace=True, random_state=1)\n",
    "test.sample(frac=0.1, replace=True, random_state=1)\n",
    "\n",
    "train.to_csv('train_sample.csv', index=False)\n",
    "test.to_csv('test_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c27446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51528/2381566190.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  app = train.append(test, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged ['INCOME_TO_ANNUITY_RATIO_ISNULL', 'INCOME_TO_ANNUITY_RATIO_BY_AGE_ISNULL', 'CREDIT_TO_ANNUITY_RATIO_ISNULL', 'CREDIT_TO_ANNUITY_RATIO_BY_AGE_ISNULL', 'AMT_ANNUITY_ISNULL'] into Merged0\n",
      "Merged ['PROPORTION_LIFE_EMPLOYED_ISNULL', 'DAYS_EMPLOYED_ISNULL'] into Merged1\n",
      "Merged ['APARTMENTS_MODE_ISNULL', 'APARTMENTS_MEDI_ISNULL', 'APARTMENTS_AVG_ISNULL'] into Merged2\n",
      "Merged ['BASEMENTAREA_MODE_ISNULL', 'BASEMENTAREA_MEDI_ISNULL', 'BASEMENTAREA_AVG_ISNULL'] into Merged3\n",
      "Merged ['YEARS_BEGINEXPLUATATION_MODE_ISNULL', 'YEARS_BEGINEXPLUATATION_MEDI_ISNULL', 'YEARS_BEGINEXPLUATATION_AVG_ISNULL'] into Merged4\n",
      "Merged ['YEARS_BUILD_MODE_ISNULL', 'YEARS_BUILD_MEDI_ISNULL', 'YEARS_BUILD_AVG_ISNULL'] into Merged5\n",
      "Merged ['COMMONAREA_MODE_ISNULL', 'COMMONAREA_MEDI_ISNULL', 'COMMONAREA_AVG_ISNULL'] into Merged6\n",
      "Merged ['ELEVATORS_MODE_ISNULL', 'ELEVATORS_MEDI_ISNULL', 'ELEVATORS_AVG_ISNULL'] into Merged7\n",
      "Merged ['ENTRANCES_MODE_ISNULL', 'ENTRANCES_MEDI_ISNULL', 'ENTRANCES_AVG_ISNULL'] into Merged8\n",
      "Merged ['FLOORSMAX_MODE_ISNULL', 'FLOORSMAX_MEDI_ISNULL', 'FLOORSMAX_AVG_ISNULL'] into Merged9\n",
      "Merged ['FLOORSMIN_MODE_ISNULL', 'FLOORSMIN_MEDI_ISNULL', 'FLOORSMIN_AVG_ISNULL'] into Merged10\n",
      "Merged ['LANDAREA_MODE_ISNULL', 'LANDAREA_MEDI_ISNULL', 'LANDAREA_AVG_ISNULL'] into Merged11\n",
      "Merged ['LIVINGAPARTMENTS_MODE_ISNULL', 'LIVINGAPARTMENTS_MEDI_ISNULL', 'LIVINGAPARTMENTS_AVG_ISNULL'] into Merged12\n",
      "Merged ['LIVINGAREA_MODE_ISNULL', 'LIVINGAREA_MEDI_ISNULL', 'LIVINGAREA_AVG_ISNULL'] into Merged13\n",
      "Merged ['NONLIVINGAPARTMENTS_MODE_ISNULL', 'NONLIVINGAPARTMENTS_MEDI_ISNULL', 'NONLIVINGAPARTMENTS_AVG_ISNULL'] into Merged14\n",
      "Merged ['NONLIVINGAREA_MODE_ISNULL', 'NONLIVINGAREA_MEDI_ISNULL', 'NONLIVINGAREA_AVG_ISNULL'] into Merged15\n",
      "Merged ['DEF_30_CNT_SOCIAL_CIRCLE_ISNULL', 'OBS_60_CNT_SOCIAL_CIRCLE_ISNULL', 'DEF_60_CNT_SOCIAL_CIRCLE_ISNULL', 'OBS_30_CNT_SOCIAL_CIRCLE_ISNULL'] into Merged16\n",
      "Merged ['AMT_REQ_CREDIT_BUREAU_DAY_ISNULL', 'AMT_REQ_CREDIT_BUREAU_WEEK_ISNULL', 'AMT_REQ_CREDIT_BUREAU_MON_ISNULL', 'AMT_REQ_CREDIT_BUREAU_QRT_ISNULL', 'AMT_REQ_CREDIT_BUREAU_YEAR_ISNULL', 'AMT_REQ_CREDIT_BUREAU_HOUR_ISNULL'] into Merged17\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, make_scorer, fbeta_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from hashlib import sha256\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Plot settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set()\n",
    "\n",
    "# Load applications data\n",
    "path = ''\n",
    "train = pd.read_csv(path + 'train_sample.csv')\n",
    "test = pd.read_csv(path + 'test_sample.csv')\n",
    "\n",
    "# Merge test and train into all application data\n",
    "train_o = train.copy()\n",
    "train['Test'] = False\n",
    "test['Test'] = True\n",
    "test['TARGET'] = np.nan\n",
    "app = train.append(test, ignore_index=True)\n",
    "\n",
    "# Remove entries with gender = XNA\n",
    "app = app[app['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "# Remove entries with income type = maternity leave\n",
    "app = app[app['NAME_INCOME_TYPE'] != 'Maternity leave']\n",
    "\n",
    "# Remove entries with unknown family status\n",
    "app = app[app['NAME_FAMILY_STATUS'] != 'Unknown']\n",
    "\n",
    "app['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "app['PROPORTION_LIFE_EMPLOYED'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\n",
    "app['INCOME_TO_CREDIT_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_CREDIT'] \n",
    "app['INCOME_TO_ANNUITY_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_ANNUITY']\n",
    "app['INCOME_TO_ANNUITY_RATIO_BY_AGE'] = app['INCOME_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\n",
    "app['CREDIT_TO_ANNUITY_RATIO'] = app['AMT_CREDIT'] / app['AMT_ANNUITY']\n",
    "app['CREDIT_TO_ANNUITY_RATIO_BY_AGE'] = app['CREDIT_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\n",
    "app['INCOME_TO_FAMILYSIZE_RATIO'] = app['AMT_INCOME_TOTAL'] / app['CNT_FAM_MEMBERS']\n",
    "\n",
    "# Add indicator columns for empty values\n",
    "for col in app:\n",
    "    if col!='Test' and col!='TARGET':\n",
    "        app_null = app[col].isnull()\n",
    "        if app_null.sum()>0:\n",
    "            app[col+'_ISNULL'] = app_null\n",
    "            \n",
    "# Label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Label encode binary fearures in training set\n",
    "for col in app: \n",
    "    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()==2:\n",
    "        if col+'_ISNULL' in app.columns: #missing values here?\n",
    "            app.loc[app[col+'_ISNULL'], col] = 'NaN'\n",
    "        app[col] = le.fit_transform(app[col])\n",
    "        if col+'_ISNULL' in app.columns: #re-remove missing vals\n",
    "            app.loc[app[col+'_ISNULL'], col] = np.nan\n",
    "            \n",
    "# Get categorical features to encode\n",
    "cat_features = []\n",
    "for col in app: \n",
    "    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()>2:\n",
    "        cat_features.append(col)\n",
    "\n",
    "# One-hot encode categorical features in train set\n",
    "app = pd.get_dummies(app, columns=cat_features)\n",
    "\n",
    "# Hash columns\n",
    "hashes = dict()\n",
    "for col in app:\n",
    "    hashes[col] = sha256(app[col].values).hexdigest()\n",
    "    \n",
    "# Get list of duplicate column lists\n",
    "Ncol = app.shape[1] #number of columns\n",
    "dup_list = []\n",
    "dup_labels = -np.ones(Ncol)\n",
    "for i1 in range(Ncol):\n",
    "    if dup_labels[i1]<0: #if not already merged,\n",
    "        col1 = app.columns[i1]\n",
    "        t_dup = [] #list of duplicates matching col1\n",
    "        for i2 in range(i1+1, Ncol):\n",
    "            col2 = app.columns[i2]\n",
    "            if ( dup_labels[i2]<0 #not already merged\n",
    "                 and hashes[col1]==hashes[col2] #hashes match\n",
    "                 and app[col1].equals(app[col2])): #cols are equal\n",
    "                #then this is actually a duplicate\n",
    "                t_dup.append(col2)\n",
    "                dup_labels[i2] = i1\n",
    "        if len(t_dup)>0: #duplicates of col1 were found!\n",
    "            t_dup.append(col1)\n",
    "            dup_list.append(t_dup)\n",
    "        \n",
    "# Merge duplicate columns\n",
    "for iM in range(len(dup_list)):\n",
    "    new_name = 'Merged'+str(iM)\n",
    "    app[new_name] = app[dup_list[iM][0]].copy()\n",
    "    app.drop(columns=dup_list[iM], inplace=True)\n",
    "    print('Merged', dup_list[iM], 'into', new_name)\n",
    "    \n",
    "# Split data back into test + train\n",
    "train = app.loc[~app['Test'], :]\n",
    "test = app.loc[app['Test'], :]\n",
    "\n",
    "train=train[['SK_ID_CURR','EXT_SOURCE_3','EXT_SOURCE_2','EXT_SOURCE_1','DAYS_EMPLOYED','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY']]\n",
    "\n",
    "train.to_csv('train_clean.csv', index=False)\n",
    "test.to_csv('test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10775360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>0.139376</td>\n",
       "      <td>0.262949</td>\n",
       "      <td>0.083037</td>\n",
       "      <td>-637.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>0.311267</td>\n",
       "      <td>-1188.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>0.555912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.650442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3039.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3038.0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>456251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681632</td>\n",
       "      <td>0.145570</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>254700.0</td>\n",
       "      <td>27558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307507</th>\n",
       "      <td>456252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>269550.0</td>\n",
       "      <td>12001.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307508</th>\n",
       "      <td>456253</td>\n",
       "      <td>0.218859</td>\n",
       "      <td>0.535722</td>\n",
       "      <td>0.744026</td>\n",
       "      <td>-7921.0</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>677664.0</td>\n",
       "      <td>29979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307509</th>\n",
       "      <td>456254</td>\n",
       "      <td>0.661024</td>\n",
       "      <td>0.514163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4786.0</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>370107.0</td>\n",
       "      <td>20205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307510</th>\n",
       "      <td>456255</td>\n",
       "      <td>0.113922</td>\n",
       "      <td>0.708569</td>\n",
       "      <td>0.734460</td>\n",
       "      <td>-1262.0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>49117.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307500 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR  EXT_SOURCE_3  EXT_SOURCE_2  EXT_SOURCE_1  DAYS_EMPLOYED  \\\n",
       "0           100002      0.139376      0.262949      0.083037         -637.0   \n",
       "1           100003           NaN      0.622246      0.311267        -1188.0   \n",
       "2           100004      0.729567      0.555912           NaN         -225.0   \n",
       "3           100006           NaN      0.650442           NaN        -3039.0   \n",
       "4           100007           NaN      0.322738           NaN        -3038.0   \n",
       "...            ...           ...           ...           ...            ...   \n",
       "307506      456251           NaN      0.681632      0.145570         -236.0   \n",
       "307507      456252           NaN      0.115992           NaN            NaN   \n",
       "307508      456253      0.218859      0.535722      0.744026        -7921.0   \n",
       "307509      456254      0.661024      0.514163           NaN        -4786.0   \n",
       "307510      456255      0.113922      0.708569      0.734460        -1262.0   \n",
       "\n",
       "        AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \n",
       "0               202500.0    406597.5      24700.5  \n",
       "1               270000.0   1293502.5      35698.5  \n",
       "2                67500.0    135000.0       6750.0  \n",
       "3               135000.0    312682.5      29686.5  \n",
       "4               121500.0    513000.0      21865.5  \n",
       "...                  ...         ...          ...  \n",
       "307506          157500.0    254700.0      27558.0  \n",
       "307507           72000.0    269550.0      12001.5  \n",
       "307508          153000.0    677664.0      29979.0  \n",
       "307509          171000.0    370107.0      20205.0  \n",
       "307510          157500.0    675000.0      49117.5  \n",
       "\n",
       "[307500 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a8e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51528/1172781550.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  app = train.append(test, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, make_scorer, fbeta_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from hashlib import sha256\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Plot settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set()\n",
    "\n",
    "# Load applications data\n",
    "path = ''#'dataset/'\n",
    "train = pd.read_csv(path + 'train_sample.csv')\n",
    "test = pd.read_csv(path + 'test_sample.csv')\n",
    "train_ids=train['SK_ID_CURR']\n",
    "test_ids=test['SK_ID_CURR']\n",
    "\n",
    "\n",
    "# Merge test and train into all application data\n",
    "train_o = train.copy()\n",
    "train['Test'] = False\n",
    "test['Test'] = True\n",
    "test['TARGET'] = np.nan\n",
    "app = train.append(test, ignore_index=True)\n",
    "\n",
    "# Remove entries with gender = XNA\n",
    "app = app[app['CODE_GENDER'] != 'XNA']\n",
    "# Remove entries with income type = maternity leave\n",
    "app = app[app['NAME_INCOME_TYPE'] != 'Maternity leave']\n",
    "# Remove entries with unknown family status\n",
    "app = app[app['NAME_FAMILY_STATUS'] != 'Unknown']\n",
    "app['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "app['PROPORTION_LIFE_EMPLOYED'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\n",
    "app['INCOME_TO_CREDIT_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_CREDIT'] \n",
    "app['INCOME_TO_ANNUITY_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_ANNUITY']\n",
    "app['INCOME_TO_ANNUITY_RATIO_BY_AGE'] = app['INCOME_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\n",
    "app['CREDIT_TO_ANNUITY_RATIO'] = app['AMT_CREDIT'] / app['AMT_ANNUITY']\n",
    "app['CREDIT_TO_ANNUITY_RATIO_BY_AGE'] = app['CREDIT_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\n",
    "app['INCOME_TO_FAMILYSIZE_RATIO'] = app['AMT_INCOME_TOTAL'] / app['CNT_FAM_MEMBERS']\n",
    "\n",
    "# Add indicator columns for empty values\n",
    "for col in app:\n",
    "    if col!='Test' and col!='TARGET':\n",
    "        app_null = app[col].isnull()\n",
    "        if app_null.sum()>0:\n",
    "            app[col+'_ISNULL'] = app_null\n",
    "\n",
    "# Label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Label encode binary fearures in training set\n",
    "for col in app: \n",
    "    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()==2:\n",
    "        if col+'_ISNULL' in app.columns: #missing values here?\n",
    "            app.loc[app[col+'_ISNULL'], col] = 'NaN'\n",
    "        app[col] = le.fit_transform(app[col])\n",
    "        if col+'_ISNULL' in app.columns: #re-remove missing vals\n",
    "            app.loc[app[col+'_ISNULL'], col] = np.nan            \n",
    "\n",
    "# Get categorical features to encode\n",
    "cat_features = []\n",
    "for col in app: \n",
    "    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()>2:\n",
    "        cat_features.append(col)\n",
    "\n",
    "# One-hot encode categorical features in train set\n",
    "app = pd.get_dummies(app, columns=cat_features)\n",
    "\n",
    "# Hash columns\n",
    "hashes = dict()\n",
    "for col in app:\n",
    "    hashes[col] = sha256(app[col].values).hexdigest()\n",
    "    \n",
    "# Get list of duplicate column lists\n",
    "Ncol = app.shape[1] #number of columns\n",
    "dup_list = []\n",
    "dup_labels = -np.ones(Ncol)\n",
    "for i1 in range(Ncol):\n",
    "    if dup_labels[i1]<0: #if not already merged,\n",
    "        col1 = app.columns[i1]\n",
    "        t_dup = [] #list of duplicates matching col1\n",
    "        for i2 in range(i1+1, Ncol):\n",
    "            col2 = app.columns[i2]\n",
    "            if ( dup_labels[i2]<0 #not already merged\n",
    "                 and hashes[col1]==hashes[col2] #hashes match\n",
    "                 and app[col1].equals(app[col2])): #cols are equal\n",
    "                #then this is actually a duplicate\n",
    "                t_dup.append(col2)\n",
    "                dup_labels[i2] = i1\n",
    "        if len(t_dup)>0: #duplicates of col1 were found!\n",
    "            t_dup.append(col1)\n",
    "            dup_list.append(t_dup)\n",
    "        \n",
    "# Merge duplicate columns\n",
    "for iM in range(len(dup_list)):\n",
    "    new_name = 'Merged'+str(iM)\n",
    "    app[new_name] = app[dup_list[iM][0]].copy()\n",
    "    app.drop(columns=dup_list[iM], inplace=True)\n",
    "    #print('Merged', dup_list[iM], 'into', new_name)\n",
    "\n",
    "# Split data back into test + train\n",
    "train = app.loc[~app['Test'], :]\n",
    "test = app.loc[app['Test'], :]\n",
    "\n",
    "# Ensure all data is stored as floats\n",
    "train = train.astype(np.float32)\n",
    "test = test.astype(np.float32)\n",
    "\n",
    "# Target labels\n",
    "train_y = train['TARGET']\n",
    "\n",
    "# Remove test/train indicator column and target column\n",
    "train.drop(columns=['Test', 'TARGET'], inplace=True)\n",
    "test.drop(columns=['Test', 'TARGET'], inplace=True)\n",
    "\n",
    "test.reset_index(inplace=True)\n",
    "train.reset_index(inplace=True)\n",
    "\n",
    "test_orig = pd.read_csv(path + 'test_sample.csv')\n",
    "train['ID']=train_ids\n",
    "#test.drop(columns=['SK_ID_CURR'])\n",
    "test['ID']=test_ids\n",
    "\n",
    "# Make SK_ID_CURR the index\n",
    "train.set_index('ID', inplace=True)\n",
    "test.set_index('ID', inplace=True)\n",
    "\n",
    "train.drop(columns=['index'], inplace=True)\n",
    "test.drop(columns=['index'], inplace=True)\n",
    "\n",
    "train.to_csv(path +'train_encoded.csv', index=False)\n",
    "# test_encoded=test[['SK_ID_CURR','EXT_SOURCE_3','EXT_SOURCE_2','EXT_SOURCE_1','DAYS_EMPLOYED','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY']]\n",
    "# test_encoded.to_csv(path +'test_encoded_clean.csv', index=False)\n",
    "test.to_csv(path +'test_encoded.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
